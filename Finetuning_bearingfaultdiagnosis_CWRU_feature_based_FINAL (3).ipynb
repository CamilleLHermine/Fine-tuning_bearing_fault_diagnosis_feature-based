{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install transformers==4.44.0 #for compatibility with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import zipfile\n",
    "import glob\n",
    "from scipy.io import loadmat # To read .mat files\n",
    "\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "import accelerate\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset, load_from_disk, concatenate_datasets, Features, Value, Sequence, ClassLabel\n",
    "import datasets\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "\n",
    "# Others\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "typAEwe8BHRu"
   },
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = '/content/drive/MyDrive/CWRUDataset.zip'\n",
    "extract_dir = 'extracted_mat_files/'\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls '/content/extracted_mat_files/CWRU Dataset/Data/12k_FE'\n",
    "!ls '/content/extracted_mat_files/CWRU Dataset/Data/48k_DE'\n",
    "!ls '/content/extracted_mat_files/CWRU Dataset/Data/12k_DE'\n",
    "!ls '/content/extracted_mat_files/CWRU Dataset/Data/Normal'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_data = loadmat('/content/extracted_mat_files/CWRU Dataset/Data/48k_DE/B007_0.mat')\n",
    "mat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_mat(file_path, key=None):\n",
    "    mat_data = loadmat(file_path)\n",
    "    # Remove metadata keys that start with \"__\"\n",
    "    mat_keys = [k for k in mat_data.keys() if not k.startswith(\"__\")]\n",
    "\n",
    "    if key is None:\n",
    "        # Pick first array-like key\n",
    "        key = mat_keys[0]\n",
    "\n",
    "    data = mat_data[key]\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "    else:\n",
    "        data = data[:, 0].reshape(-1, 1)\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    if os.path.basename(file_path).startswith('B'):\n",
    "      label=('Ball')\n",
    "    elif os.path.basename(file_path).startswith('IR'):\n",
    "      label=('Inner race')\n",
    "    elif os.path.basename(file_path).startswith('OR'):\n",
    "      label=('Outer race')\n",
    "    elif os.path.basename(file_path).startswith('Normal'):\n",
    "      label=('Normal')\n",
    "\n",
    "\n",
    "\n",
    "    return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_folder(folder_path, key=None):\n",
    "    all_data = []\n",
    "    labels=[]\n",
    "    if os.path.basename(folder_path).startswith('12'):\n",
    "      freq=12\n",
    "    else:\n",
    "      freq=48\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".mat\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data,label = extract_data_from_mat(file_path, key=key)\n",
    "            all_data.append(data)\n",
    "            labels.append(label)\n",
    "\n",
    "    return all_data,labels,freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/content/extracted_mat_files/CWRU Dataset/Data/48k_DE/', '/content/extracted_mat_files/CWRU Dataset/Data/12k_DE/', '/content/extracted_mat_files/CWRU Dataset/Data/12k_FE/','/content/extracted_mat_files/CWRU Dataset/Data/Normal/']\n",
    "all_data = []\n",
    "all_labels = []\n",
    "freq = []\n",
    "for folder in file_paths:\n",
    "    data,labels,frequence = load_folder(folder)\n",
    "    if all_data:\n",
    "      all_data=all_data+data\n",
    "      all_labels=all_labels+labels\n",
    "      freq=freq+[frequence] * len(all_labels)\n",
    "\n",
    "    else:\n",
    "      all_data=data\n",
    "      all_labels=labels\n",
    "      freq=[frequence] * len(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data[0:5], len(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjQoGok1Qyjc"
   },
   "source": [
    "Divide in windows of 2048 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=2048\n",
    "step=512\n",
    "input_chunk=[]\n",
    "target_chunk=[]\n",
    "final_freq=[]\n",
    "for i in range(0,len(all_labels)):\n",
    "  for j in range(0,len(all_data[i])-max_length,step):\n",
    "    if len(all_data[i][j:j+max_length])>0:\n",
    "      input_chunk.append(all_data[i][j:j+max_length])\n",
    "      target_chunk.append(all_labels[i])\n",
    "      final_freq.append(freq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_chunk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(target_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chunk=np.array(input_chunk)\n",
    "final_freq=np.array(final_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(x, fs):\n",
    "\n",
    "    N = len(x)\n",
    "    # Fourier transform\n",
    "    fft_vals = np.fft.fft(x)\n",
    "    fft_freq = np.fft.fftfreq(N, 1/fs)\n",
    "    # Use only the positive frequency components since symmetric\n",
    "    K = N // 2\n",
    "    s = np.abs(fft_vals[:K])\n",
    "    f = fft_freq[:K]\n",
    "\n",
    "    # Time-Domain Features\n",
    "    p1 = np.mean(x)\n",
    "    p2 = np.std(x, ddof=1) # ddof=1 for sample standard deviation\n",
    "    p3 = (np.mean(np.sqrt(np.abs(x))))**2\n",
    "    p4 = np.mean(np.abs(x))\n",
    "    p5 = np.max(np.abs(x))\n",
    "    p6 = np.mean(x**3)\n",
    "    p7 = np.mean(x**4)\n",
    "    p8 = np.mean(x**2)\n",
    "    p9 = p7 / np.abs(p6) #WEIRD\n",
    "    p10 = p5 / p2\n",
    "    p11 = p2 / p4\n",
    "    p12 = p5 / p4\n",
    "    p13 = np.mean(s)\n",
    "    p14 = np.var(s, ddof=1)\n",
    "    p15 = (1 / (K * p14**(3/2))) * np.sum((s - p13)**3)\n",
    "    p16 = (1 / (K * p14**2)) * np.sum((s - p13)**4)\n",
    "    p17 = np.sum(f * s) / np.sum(s)\n",
    "    p18 = np.sqrt(np.sum(((f - p17)**2) * s) / K*np.sum(s))\n",
    "    p19 = np.sqrt(np.sum((f**2) * s) / np.sum(s))\n",
    "    p20 = np.sqrt(np.sum(f**4 * s) / np.sum(f**2 * s))\n",
    "    p21 = np.sum(f**2 * s) / np.sqrt(np.sum(s) / np.sum(f**4 * s))\n",
    "    p22 = p18 / p17\n",
    "    p23 = np.sum((f - p17)**3 * s) / (K * p18**3)\n",
    "    p24 = np.sum((f - p17)**4 * s) / (K * p18**4)\n",
    "\n",
    "    feat=[p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,p17,p18,p19,p20,p21,p22,p23,p24]\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk=2500\n",
    "Features1=[]\n",
    "for i in range(0,len(input_chunk)):\n",
    "  Features1.append(calculate_features(input_chunk[i],final_freq[i]))\n",
    "  #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompting(features=Features1):\n",
    "  t='time'\n",
    "  f='frequency'\n",
    "  feat=[]\n",
    "  for feature in features:\n",
    "    prompt0= \"You are a bearing fault diagnosis expert. Based on the following features, you need to conduct fault diagnosis:\"\n",
    "    prompt1=f' The mean value of the vibration signal in the {t} domain is {feature[0]}.'\n",
    "    prompt2=f' The standard deviation of the vibration signal in the {t} domain is {feature[1]}.'\n",
    "    prompt3=f' The square root amplitude of the vibration signal in the {t} domain is {feature[2]}.'\n",
    "    prompt4=f' The absolute mean value of the vibration signal in the {t} domain is {feature[3]}.'\n",
    "    prompt5=f' The peak value of the vibration signal in the {t} domain is {feature[4]}.'\n",
    "    prompt6=f' The skewness of the vibration signal in the {t} domain is {feature[5]}.'\n",
    "    prompt7=f' The kurtosis of the vibration signal in the {t} domain is {feature[6]}.'\n",
    "    prompt8=f' The variance of the vibration signal in the {t} domain is {feature[7]}.'\n",
    "    prompt9=f' The kurtosis index of the vibration signal in the {t} domain is {feature[8]}.'\n",
    "    prompt10=f' The peak index of the vibration signal in the {t} domain is {feature[9]}.'\n",
    "    prompt11=f' The waveform index of the vibration signal in the {t} domain is {feature[10]}.'\n",
    "    prompt12=f' The pulse index of the vibration signal in the {t} domain is {feature[11]}.'\n",
    "    prompt13=f' The frequency mean value of the vibration signal in the {f} domain is {feature[12]}.'\n",
    "    prompt14=f' The frequency variance of the vibration signal in the {f} domain is {feature[13]}.'\n",
    "    prompt15=f' The frequency skewness of the vibration signal in the {f} domain is {feature[14]}.'\n",
    "    prompt16=f' The frequency kurtosis of the vibration signal in the {f} domain is {feature[15]}.'\n",
    "    prompt17=f' The gravity frequency of the vibration signal in the {f} domain is {feature[16]}.'\n",
    "    prompt18=f' The frequency standard deviation of the vibration signal in the {f} domain is {feature[17]}.'\n",
    "    prompt19=f' The frequency root mean square of the vibration signal in the {f} domain is {feature[18]}.'\n",
    "    prompt20=f' The average frequency of the vibration signal in the {f} domain is {feature[19]}.'\n",
    "    prompt21=f' The regularity degree of the vibration signal in the {f} domain is {feature[20]}.'\n",
    "    prompt22=f' The variation parameter of the vibration signal in the {f} domain is {feature[21]}.'\n",
    "    prompt23=f' The eighth-order moment of the vibration signal in the {f} domain is {feature[22]}.'\n",
    "    prompt24=f' The sixteenth order moment of the vibration signal in the {f} domain is {feature[23]}.'\n",
    "\n",
    "    feat.append(prompt0+prompt1+prompt2+prompt3+prompt4+prompt5+prompt6+prompt7+prompt8+prompt9+prompt10+prompt11+prompt12+prompt13+prompt14+prompt15+prompt16+prompt17+prompt18+prompt19+prompt20+prompt21+prompt22+prompt23+prompt24)\n",
    "  return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "numeric_labels = encoder.fit_transform(target_chunk)\n",
    "\n",
    "print(f\"Classes (mapping): {encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=prompting()\n",
    "y=numeric_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,    # 20% for testing\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"THUDM/chatglm2-6b\"\n",
    "num_classes = 4\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels, tokenizer=tokenizer, max_length=2048):\n",
    "        assert len(features) == len(labels)\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.features[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None\n",
    "          )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Training DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Shuffles data for every epoch\n",
    "    drop_last=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Testing DataLoader\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_to_text(features):\n",
    "    return \" \".join(map(str, features))\n",
    "\n",
    "train_texts = [feature_to_text(f) for f in X_train]\n",
    "test_texts = [feature_to_text(f) for f in X_test]\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, y_train)\n",
    "test_dataset = CustomDataset(test_texts, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=8,  # Rank of the update matrices\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#Reinitialization because there were NaN in the classifier head\n",
    "for name, module in peft_model.named_modules():\n",
    "    if \"classifier_head\" in name:\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "        module.apply(_init_weights)\n",
    "        print(\"Reinitialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"\\nVerifying weights are now valid...\")\n",
    "# Check if still NaN parameters in the classifier_head\n",
    "for n, p in peft_model.named_parameters():\n",
    "    if \"classifier_head\" in n:\n",
    "        has_nan = torch.isnan(p).any().item()\n",
    "        print(f\"Parameter '{n}' has NaN: {has_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=10,                # How often to log training progress\n",
    "    save_steps=50,                   # How often to save a checkpoint\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # The model outputs logits, so we take the argmax to get the predicted class\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)   # [batch_size, seq_len]\n",
    "print(batch['labels'].shape)      # [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"/content/drive/MyDrive/lora-adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "\n",
    "print(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
